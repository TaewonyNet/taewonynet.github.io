---
title: requests를 이용하여 웹사이트 정보를 획득해보자
description: 간단히 웹사이트 데이터를 획득해보자
author: taewony
date: 2026-01-22 23:42:15 +0900
categories: [Development, Tech/Architecture]
tags: [python, requests, beautifulsoup, crawling, web-scraping, metadata]
pin: false
math: false
mermaid: false
---

## requests를 이용하여 웹사이트 정보를 획득해보자
간단히 웹사이트 데이터를 획득해보자

### 1. 서론 (Introduction)

- **문제/상황 (Problem):**
    - 특정 URL의 제목, 설명, Open Graph 데이터 등 핵심 정보를 수동으로 확인하는 것은 비효율적이다.
- **목적 (Purpose):**
    - requests와 BeautifulSoup을 활용하여 웹사이트의 메타 데이터를 자동으로 추출하는 간결한 클라이언트를 구현한다.
- **대상 (Target Audience):**
    - 웹 데이터를 수집해야 하는 데이터 엔지니어나 간단한 크롤러를 제작하려는 개발자

### 2. 방법 및 과정 (Methods & Process)

- **배경 조사 및 데이터 (Data Collection):**
    - HTTP 요청을 위한 표준적인 requests 라이브러리와 HTML 파싱을 위한 BeautifulSoup4의 안정성을 확인하였다.

| 기능                       | 설명                                      | 장점                                   |
| -------------------------- | ----------------------------------------- | -------------------------------------- |
| **Session 관리**           | 쿠키 및 헤더 유지를 위한 Session 객체 사용 | 로그인 상태 유지 및 효율적인 연결 관리 |
| **User-Agent 로테이션**    | 무작위 User-Agent 헤더 적용               | 기본적인 봇 차단 메커니즘 회피         |
| **HTML 파싱**              | CSS 선택자 및 태그 탐색 활용              | 원하는 데이터를 정확하고 빠르게 추출   |
| **에러 및 재시도**         | 네트워크 예외 처리 및 재시도 로직 포함    | 일시적인 네트워크 불안정에도 유연하게 대응 |
| **메타 데이터 통합**       | Title, Description, OG 태그 일괄 수집     | 웹사이트의 핵심 정보를 구조화된 데이터로 변환 |

- **접근 방법 (Approach Methods):**
    - **[방법 1]:** requests.Session을 생성하여 일관된 요청 환경을 설정하고 로그인을 시뮬레이션한다.
    - **[방법 2]:** BeautifulSoup을 사용하여 HTML 구조 내에서 필요한 메타 태그를 탐색하고 추출한다.

- **분석 및 해결 프로세스 (Analysis Flow):**
    - **도구/기술:** Python, requests, beautifulsoup4, Flask(데모 서버용)
    - **주요 단계:** 종속성 체크 모듈 연동 $\rightarrow$ WebClient 클래스 설계 $\rightarrow$ 세션 및 헤더 설정 $\rightarrow$ HTML 파싱 로직 구현 $\rightarrow$ 로컬 서버를 통한 기능 검증
    - **결과 도출 및 검증:** 데모 서버에 접속하여 게스트 및 로그인 사용자 상태에서의 메타 데이터 추출 결과가 일치함을 확인한다.

### 3. 결과 (Results)

- **분석 결과 요약:**
    - requests 라이브러리를 통해 동기식으로 웹 데이터를 안정적으로 획득할 수 있음을 확인하였다.
    - BeautifulSoup4를 결합하여 복잡한 HTML 문서에서도 원하는 메타 정보를 사전(Dict) 형태로 구조화하는 프로세스를 정립하였다.

### 4. 인사이트 및 액션 (Insights & Action)

- **인사이트 (Insight):**
    - 사이트 정보를 획득하는 간단한 모듈을 사용해보았다. 다음엔 비동기로 데이터를 획득해보자.
    - 단순한 정적 페이지 크롤링에는 requests가 가장 비용 효율적이고 안정적인 선택지이다.
- **실행 방안 (Action Plan):**
    - 구현된 클라이언트를 라이브러리화하여 소규모 스크래핑 프로젝트에 공통 모듈로 적용한다.
- **한 줄 결론 (Key Takeaway):**
    - requests와 BeautifulSoup의 조합으로 누구나 쉽게 웹 메타 데이터를 자동 수집할 수 있다. [샘플 코드](https://github.com/TaewonyNet/taewonynet.github.io/blob/master/src/webclient_rq.py){: target="_blank"}
- **다음 스텝 (Next Step):**
    - 대량의 URL 처리를 위해 httpx를 도입한 비동기 크롤링 방식으로 성능을 개선한다.

---
title: Ollama를 사용해보자
description: 로컬에서 돌릴 수 있는 LLM인 Ollama를 사용해보자
author: taewony
date: 2025-01-07 22:10:16 +0900
categories: [Development, AI/Data]
tags: [ollama, local llm, open source, ai development, offline ai]
pin: false
math: false
mermaid: false
---

## Ollama를 사용해보자
로컬에서 돌릴 수 있는 LLM인 Ollama를 사용해보자

### 1. 서론 (Introduction)

- **문제/상황 (Problem):**
    - 보안 이슈로 로컬 모델을 사용해보고 싶다.
- **목적 (Purpose):**
    - Ollama로 로컬 LLM 모델을 돌려 보안과 비용 절감 효과를 얻도록 한다.
- **대상 (Target Audience):**
    - 보안 이슈가 있거나 비용 절감이 필요한 개발자

### 2. 방법 및 과정 (Methods & Process)

- **배경 조사 및 데이터 (Data Collection):**
    - 로컬 LLM 실행 가능한 도구를 조사한 후 설치하기 쉽고 API 연동이 쉬운 Ollama가 있음을 확인했다.

| VRAM 대역 | 모델 이름                        | 대략 VRAM(Q4 기준) | 특성 / 용도 요약                                    |
| ------- | ---------------------------- | -------------- | --------------------------------------------- |
| 4–8GB   | phi4:mini (또는 3B/4B 계열)      | ~4–6GB         | 초경량 추론·일반 대화용, 교육·튜토리얼 스타일 답변에 적합.            |
| 4–8GB   | phi-3.5-mini:3.8b            | ~4–6GB         | 작은 사이즈 대비 강한 추론·코딩, 온디바이스 실험용으로 자주 사용.        |
| 4–8GB   | gemma3:4b                    | ~4–6GB         | 빠른 응답, 간단 요약·채팅·질문응답용 경량 모델.                  |
| 4–8GB   | llama3.2:3b                  | ~5–6GB         | Meta 초경량 영어 중심 모델, 간단 업무 자동화·도우미 봇에 적합.       |
| 4–8GB   | mistral:7b                   | ~6–8GB         | 7B 클래식, 짧은 답변·간단 코딩·요약에 무난한 범용 모델.            |
| 4–8GB   | qwen2.5:7b                   | ~6–8GB         | 다국어 지원 우수, 한국어 포함 일반 QA·번역·요약에 적합.            |
| 8–12GB  | llama3.1:8b                  | ~8–10GB        | Meta 8B 플래그십, 안정적인 영어 대화와 적당한 코딩 성능.          |
| 8–12GB  | llama3.3:8b                  | ~8–10GB        | 개선된 학습 데이터와 향상된 추론, 플러그인·에이전트 실험에 자주 사용.      |
| 8–12GB  | mistral-small-3.1:8b         | ~9–11GB        | 상용급에 가까운 8B, 챗봇·요약·질문응답·간단 코드용 다목적 모델.        |
| 8–12GB  | olmo2:7b                     | ~8–10GB        | 연구 지향 영어 모델, 학술·분석 작업에 강점.                    |
| 8–12GB  | olmo2:13b                    | ~10–12GB       | 더 긴 맥락과 분석에 유리한 중형 연구용 모델.                    |
| 12–20GB | llama3.1:34b                 | ~16–20GB       | 70B 대비 가벼우면서 복잡 추론·요약·분석에 충분한 성능.             |
| 12–20GB | llama3.3:34b                 | ~16–20GB       | 최신 Meta 30B급, 멀티태스크 고성능 범용 모델.                |
| 12–20GB | mixtral:8x7b                 | ~16–20GB       | MoE 구조, 긴 문서 요약·질문응답에 효율적.                    |
| 12–20GB | command-r-plus:34b           | ~18–22GB       | 툴콜·RAG·코딩 에이전트에 강한 상위권 오픈 모델.                 |
| 12–20GB | dbrx-instruct:132b (강한 양자화)  | ~18–24GB       | 대형 파라미터를 Q4로 사용, 긴 맥락 분석·연구용 실험에 사용.          |
| 20–32GB | llama3.1:70b (Q4)            | ~24–28GB       | 고난도 수학·코딩·연구 질의에 우수한 대형 모델.                   |
| 20–32GB | command-r-plus:70b (포트 존재 시) | ~24–30GB       | 에이전트·툴 호출·코딩에 최적화된 대형 오픈 모델.                  |
| 20–32GB | qwen2.5:32b                  | ~20–24GB       | 다국어·아시아 언어와 긴 문서 요약·번역에서 강점.                  |
| 20–32GB | mistral-large (오픈 가중치 변종)    | ~24–28GB       | 코드·요약·분석에 균형 잡힌 고성능 범용 모델.                    |
| 32GB 이상 | olmo-3.1:32b                 | ~22–26GB       | 연구·평가용, 학술 텍스트와 분석 작업에 적합.                    |
| 32GB 이상 | qwen3-vl:72b                 | 30GB 이상        | 텍스트+이미지 멀티모달, 문서·도표·이미지를 함께 다루는 RAG에 적합.      |
| 32GB 이상 | qwen3-vl:100b+               | 40GB 이상        | 초대형 멀티모달, 고난도 멀티모달 추론·검색·분석용.                 |
| 32GB 이상 | MiniMax-M2:230b (로컬 포트 기준)   | 40GB 이상        | 코딩·에이전트 워크플로 특화 초대형 모델, 매우 고사양 환경에서 실험적으로 사용. |

- **접근 방법 (Approach Methods):**
    - **[방법 1]:** Ollama 공식 모델 목록을 조사하여 모델 별 특징을 확인한다.
    - **[방법 2]:** 사용 할 로컬 PC를 준비한 후 모델을 설치하여 테스트한다.

- **분석 및 해결 프로세스 (Analysis Flow):**
    - **도구/기술:** Python, Ollama CLI
    - **주요 단계:**
        1. [Ollama Download](https://ollama.com/download)에가서 설치한다. 
        2. [Ollama Model](https://ollama.com/library)에 가서 모델을 확인한다.
        3. 로컬에서 shell에서 `ollama run {mdoel}`형식으로 실행하면 바로 실행된다.
        4. API 형태로 쓰고 싶다면 파이썬 코드로 동작하게 하면된다.
    - **결과 도출 및 검증:** 로컬 환경에서 API 키 없이 간단한 모델로도 간단한 작업이 가능함을 확인했다.

### 3. 결과 (Results)

- **분석 결과 요약:**
    - Ollama는 아주 쉽게 설치하여 CLI 및 API를 제공하여 사용할 수 있다.
    - Llama, Qwen, DeepSeek 등 모델들을 무료로 제한 없이 사용할 수 있다.

### 4. 인사이트 및 액션 (Insights & Action)

- **인사이트 (Insight):**
    - 2026년 기준 로컬 LLM은 고사양이 아니더라도 충분히 활용 가능하다. 
    - 프롬프트만 정교하게 작성하면 훌륭한 프로세스를 구성할 수 있다.
- **실행 방안 (Action Plan):**
    - [Ollama 공식 사이트](https://ollama.com)에서 설치 후 Ollama run으로 모델을 실행한다.
- **한 줄 결론 (Key Takeaway):**
    - Ollama를 저사양 PC에서도 충분히 의미 있는 LLM 환경 구축이 가능하다. [샘플 코드](https://github.com/TaewonyNet/taewonynet.github.io/blob/master/src/ollama.py)
- **다음 스텝 (Next Step):**
    - Ollama에 웹을 연결해보자.
